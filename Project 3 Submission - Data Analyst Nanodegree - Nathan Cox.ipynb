{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project 3 Submission- Data Analyst Nanodegree - by Nathan Cox\n",
    "# Open Streetmap Data Wrangling with MongoDB\n",
    "#### Map Area: Toronto, Ontario\n",
    "\n",
    "https://mapzen.com/data/metro-extracts/metro/toronto_canada/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Data Wrangling\n",
    "\n",
    "### 1.1 Data Acquisition\n",
    "After downloading the Toronto, Canada dataset from mapzen.com (link above), I extracted the file into the project directory on my machine. In order speed up the investigation step, I made a subset of the file using the following code. It copies every *kth* tag into a new .osm file making the subset file *1/k* times the size of the original file. The code was provided in the Project Details section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET  #library for processing xml\n",
    "\n",
    "OSM_FILE = \"toronto_canada.osm\"  #the full .osm Toronto, Canada dataset (input)\n",
    "SAMPLE_FILE = \"toronto_small.osm\" #the subset .osm dataset (output)\n",
    "\n",
    "k = 10 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2 Data Investigation and Cleaning\n",
    "Using the reduced dataset, I investigated the following fields to determine the quality of the data and improve it where I could.\n",
    "\n",
    "#### 1.2.1 Investigating Street Names\n",
    "To investigate street names, I ran through each element that had a street name tag and inspected the street name. If the street name was not one of the *expected* street names, it was added to a list of unexpected street names. This list was then inspected manually and the following 2 actions were taken:\n",
    "\n",
    "1. The list of *expected* street names was updated with any valid street names that were found but were not already on the *expected* list.\n",
    "2. A seperate list was created for street names that were in an incorrect format. This list could then be used to clean the data.\n",
    "\n",
    "Below is the code used to inspect the data. The 'expected' list is a list of the street names that are in the correct format. The 'mapping' dictionary is a list of street names that need to be corrected. This dictionary is used in the next step to clean the data before entering it into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "OSMFILE = \"toronto_small.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"West\", \"East\", \"North\", \"South\", \"Way\", \"Townline\", \"Terrace\",\n",
    "            \"Wood\", \"Walk\", \"Vineway\", \"Sideroad\", \"Run\", \"Row\", \"Ridge\", \"Starway\", \"Promenade\", \"Path\", \n",
    "            \"Pathway\", \"Point\", \"Park\", \"Millway\", \"Mews\", \"Meadoway\", \"Manor\", \"Line\", \"Landing\", \n",
    "            \"Hollow\", \"Hill\", \"Highway\", \"Heights\", \"Grove\", \"Gate\", \"Gardens\", \"Crescent\", \"Fernway\", \n",
    "            \"Crossing\", \"Garden\", \"Common\", \"Concession\", \"Close\", \"Circuit\", \"Circle\" ]\n",
    "\n",
    "# this variable is compiled by inspecting the program output and adding incorrect street name formats to the dictionary\n",
    "mapping = {     \"St\": \"Street\",\n",
    "                \"St.\": \"Street\",\n",
    "                \"Ave\": \"Avenue\",\n",
    "                \"Ave.\" : \"Avenue\",\n",
    "                \"Rd.\": \"Road\",\n",
    "                \"Rd\" : \"Road\",\n",
    "                \"W.\" : \"West\",\n",
    "                \"E.\" : \"East\",\n",
    "                \"N.\" : \"North\",\n",
    "                \"S.\" : \"South\",\n",
    "                \"Unionville\" : \"\",\n",
    "                \"Tottenham\" : \"\",\n",
    "                \"Hrbr\" : \"Harbour\",\n",
    "                \"Dr\" : \"Drive\",\n",
    "                \"EHS\" : \"\",\n",
    "                \"Cresent\" : \"Crescent\",\n",
    "                \"By-pass\" : \"Bypass\",\n",
    "                \"Blvd\" : \"Boulevard\",\n",
    "                \"Blvd.\" : \"Boulevard\",\n",
    "                \"Alliston\" : \"\" }\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    \n",
    "    osm_file.close()\n",
    "    print \"Audit complete\"\n",
    "    return street_types\n",
    "\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    m = street_type_re.search(name) #extracts the street name from the street address string\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type in mapping:\n",
    "            name = re.sub(street_type_re, mapping[street_type], name)\n",
    "            \n",
    "    return name\n",
    "\n",
    "\n",
    "def test():\n",
    "    st_types = audit(OSMFILE)\n",
    "    pprint.pprint(dict(st_types)) #prints out the list of unexpected street names for visual inspection\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Investigating Postal Codes\n",
    "To investigate postal codes (Canadian equivalent of zip codes), I ran through each element that had a \"addr:postcode\" tag and inspected the information. Postal codes are 6 characters long and they alternate a letter followed by a number(eg. A1A 1A1). The most common way to represent a postal code is to capitalize the letters and to have a space after 3 characters. A regular expression was used to determine if the postal code was entered in an expected format (A1A 1A1). The most common exception was if the postal code did not have a space (A1A1A1). Of 684 postal codes in the reduced size data set, approximated 40 had complete information but were missing the space in the middle. 4 other 'postcode' tags did not have correct information or were missing information (less than 6 characters). When the data is entered into the database, the postal codes with the missing space will be corrected to the standard format. Postal codes with missing information will be ignored.\n",
    "\n",
    "Below is the code used to inspect the data. The regular expressions used to match the 2 postal code formats will be used later to clean the data before loading into the database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "OSMFILE = \"toronto_small.osm\"\n",
    "\n",
    "#expected postal code pattern A1A 1A1\n",
    "post_code_re = re.compile(r'^[A-Z][0-9][A-Z] [0-9][A-Z][0-9]$') \n",
    "\n",
    "#most common incorrect postal code pattern A1A1A1\n",
    "post_code_re2 = re.compile(r'^[A-Z][0-9][A-Z][0-9][A-Z][0-9]$')\n",
    "\n",
    "def audit_post_code(post_code_types, post_code):\n",
    "    m = post_code_re.search(post_code)\n",
    "    n = post_code_re2.search(post_code)\n",
    "    \n",
    "    \n",
    "    if n:  #matches the most common incorrect pattern (A1A1A1) and corrects by adding a space in the middle\n",
    "        post_code = re.sub(r'^[A-Z][0-9][A-Z]', post_code[:3] + \" \", post_code)\n",
    "        print post_code      \n",
    "    elif not m: #postal codes that do not match the expected pattern (A1A 1A1) are added to the list for visual inspection    \n",
    "        if post_code not in post_code_types:\n",
    "            post_code_types[post_code] = 1\n",
    "        else:\n",
    "            post_code_types[post_code] += 1\n",
    "\n",
    "\n",
    "def is_post_code(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    post_code_types = {}\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_post_code(tag):\n",
    "                    audit_post_code(post_code_types, tag.attrib['v'])\n",
    "    \n",
    "    osm_file.close()\n",
    "    return post_code_types\n",
    "\n",
    "\n",
    "def test():\n",
    "    post_code_types = audit(OSMFILE)\n",
    "    pprint.pprint(dict(post_code_types))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 City Names\n",
    "To investigate city names, I ran through each element that had a \"addr:city\" tag and inspected the information. There was 1 major formatting exception. A prefix of \"City of\" or \"Town of\" was added to many of the city names. \n",
    "\n",
    "Below is the code used to inspect the data. A dictionary called \"mapping\" was created to capture these sections. This dictionary will be used during the data cleaning phase to ensure that data entered into the database is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "\n",
    "OSMFILE = \"toronto_small.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "# dictionary that will be used in data cleaning phase\n",
    "# compiled by visual inspection of output\n",
    "mapping = {  'City of Brampton' : 'Brampton',\n",
    "             'City of Burlington' : 'Burlington',\n",
    "             'City of Hamilton': 'Hamilton',\n",
    "             'City of Kawartha Lakes': 'Kawartha Lakes',\n",
    "             'City of Oshawa': 'Oshawa',\n",
    "             'City of Pickering': 'Pickering',\n",
    "             'City of St. Catharines': 'St. Catherines',\n",
    "             'City of Toronto': 'Toronto',\n",
    "             'City of Vaughan': 'Vaughan',\n",
    "             'Richmond Hill (Oak Ridges)': 'Richmond Hill',\n",
    "             'Town of Ajax': 'Ajax',\n",
    "             'Town of Aurora': 'Aurora',\n",
    "             'Town of Bradford West Gwillimbury': 'Bradford West Gwillimbury',\n",
    "             'Town of Caledon': 'Caledon',\n",
    "             'Town of East Gwillimbury': 'East Gwillimbury',\n",
    "             'Town of Erin': 'Erin',\n",
    "             'Town of Grimsby': 'Grimsby',\n",
    "             'Town of Halton Hills': 'Halton Hills',\n",
    "             'Town of Innisfil': 'Innisfil',\n",
    "             'Town of Markham': 'Markham',\n",
    "             'Town of Milton': 'Milton',\n",
    "             'Town of Mono': 'Mono',\n",
    "             'Town of New Tecumseth': 'New Tecumseth',\n",
    "             'Town of Newmarket': 'Newmarket',\n",
    "             'Town of Niagara-On-The-Lake': 'Niagara-On-The-Lake',\n",
    "             'Town of Whitby': 'Whitby',\n",
    "             'Town of Whitchurch-Stouffville': 'Whitchurch-Stouffville'  }\n",
    "\n",
    "def audit_city(city_types, city_name):\n",
    "    if city_name not in city_types:\n",
    "        city_types[city_name] = 1\n",
    "    else:\n",
    "        city_types[city_name] += 1\n",
    "\n",
    "\n",
    "def is_city(elem):\n",
    "    return (elem.attrib['k'] == \"addr:city\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    city_types = {}\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_city(tag):\n",
    "                    audit_city(city_types, tag.attrib['v'])\n",
    "    \n",
    "    osm_file.close()\n",
    "    return city_types\n",
    "\n",
    "\n",
    "def test():\n",
    "    city_types = audit(OSMFILE)\n",
    "    pprint.pprint(dict(city_types))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 Cleaning Data and Exporting to json \n",
    "To clean the data and get it ready for importing into the database, the .osm file is opened and each element is inspected. If the element is a 'way' or a 'node', that element is then inspected further. Relevant element attributes are stored in Python dictionaries or lists so that they can later be exported to json easily. The attributes that are collected from each element are the type (node or way), latitude, longitude, created date and address (including city name, street name, postal code and other address sub-attributes). \n",
    "\n",
    "Before storing the city name, street name and postal code attributes, the text for each attribute is cleaned. For city and street names, the attribute text is looked up in the corresponding 'mapping' dictionary that was created in the investigation phase above. If the 'mapping' dictionary has key matching the attribute text, the corresponding value pair is substituted to clean the data. For postal codes, the most common error is corrected using a regular expression.\n",
    "\n",
    "The code to clean the osm file and output a json file is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "addr = re.compile(r'(?:addr:).*')\n",
    "\n",
    "#expected postal code pattern A1A 1A1\n",
    "post_code_re = re.compile(r'^[A-Z][0-9][A-Z] [0-9][A-Z][0-9]$') \n",
    "\n",
    "#most common incorrect postal code pattern A1A1A1\n",
    "post_code_re2 = re.compile(r'^[A-Z][0-9][A-Z][0-9][A-Z][0-9]$')\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "street_mapping = {     \"St\": \"Street\",\n",
    "                \"St.\": \"Street\",\n",
    "                \"Ave\": \"Avenue\",\n",
    "                \"Ave.\" : \"Avenue\",\n",
    "                \"Rd.\": \"Road\",\n",
    "                \"Rd\" : \"Road\",\n",
    "                \"W.\" : \"West\",\n",
    "                \"E.\" : \"East\",\n",
    "                \"N.\" : \"North\",\n",
    "                \"S.\" : \"South\",\n",
    "                \"W\" : \"West\",\n",
    "                \"E\" : \"East\",\n",
    "                \"N\" : \"North\",\n",
    "                \"S\" : \"South\",\n",
    "                \"Unionville\" : \"\",\n",
    "                \"Tottenham\" : \"\",\n",
    "                \"Hrbr\" : \"Harbour\",\n",
    "                \"Dr\" : \"Drive\",\n",
    "                \"EHS\" : \"\",\n",
    "                \"Cresent\" : \"Crescent\",\n",
    "                \"By-pass\" : \"Bypass\",\n",
    "                \"Blvd\" : \"Boulevard\",\n",
    "                \"Blvd.\" : \"Boulevard\",\n",
    "                \"Alliston\" : \"\",\n",
    "                \"Adjala\" : \"\",\n",
    "                \"Beeton\" : \"\",\n",
    "                 \"By-pass\" : \"Bypass\"\n",
    "                 }\n",
    "\n",
    "\n",
    "city_mapping = {  'City of Brampton' : 'Brampton',\n",
    "             'City of Burlington' : 'Burlington',\n",
    "             'City of Hamilton': 'Hamilton',\n",
    "             'City of Kawartha Lakes': 'Kawartha Lakes',\n",
    "             'City of Oshawa': 'Oshawa',\n",
    "             'City of Pickering': 'Pickering',\n",
    "             'City of St. Catharines': 'St. Catherines',\n",
    "             'City of Toronto': 'Toronto',\n",
    "             'City of Vaughan': 'Vaughan',\n",
    "             'Richmond Hill (Oak Ridges)': 'Richmond Hill',\n",
    "             'Town of Ajax': 'Ajax',\n",
    "             'Town of Aurora': 'Aurora',\n",
    "             'Town of Bradford West Gwillimbury': 'Bradford West Gwillimbury',\n",
    "             'Town of Caledon': 'Caledon',\n",
    "             'Town of East Gwillimbury': 'East Gwillimbury',\n",
    "             'Town of Erin': 'Erin',\n",
    "             'Town of Grimsby': 'Grimsby',\n",
    "             'Town of Halton Hills': 'Halton Hills',\n",
    "             'Town of Innisfil': 'Innisfil',\n",
    "             'Town of Markham': 'Markham',\n",
    "             'Town of Milton': 'Milton',\n",
    "             'Town of Mono': 'Mono',\n",
    "             'Town of New Tecumseth': 'New Tecumseth',\n",
    "             'Town of Newmarket': 'Newmarket',\n",
    "             'Town of Niagara-On-The-Lake': 'Niagara-On-The-Lake',\n",
    "             'Town of Whitby': 'Whitby',\n",
    "             'Town of Whitchurch-Stouffville': 'Whitchurch-Stouffville'  }\n",
    "\n",
    "\n",
    "def clean_post_code(post_code):\n",
    "    \n",
    "    #matches the most common incorrect pattern (A1A1A1) and corrects by adding a space in the middle\n",
    "    if post_code_re.search(post_code):  \n",
    "        return post_code\n",
    "              \n",
    "    #postal codes that do not match the expected pattern (A1A 1A1) are added to the list for visual inspection    \n",
    "    elif post_code_re2.search(post_code): \n",
    "        return re.sub(r'^[A-Z][0-9][A-Z]', post_code[:3] + \" \", post_code)\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def clean_city(city_name):\n",
    "    if city_name in city_mapping:\n",
    "        return city_mapping[city_name]\n",
    "    else:\n",
    "        return city_name\n",
    "\n",
    "def clean_streen_names(street_name):\n",
    "    if street_name in street_mapping:\n",
    "        return street_mapping[street_name]\n",
    "    else:\n",
    "        return street_name\n",
    "\n",
    "def shape_element(element):\n",
    "    node = {}\n",
    "    node['pos']=[]\n",
    "    \n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        node['type'] = element.tag\n",
    "        \n",
    "        if 'lat' in element.attrib:\n",
    "            node['pos'].append(float(element.attrib['lat']))\n",
    "            \n",
    "            if 'lon' in element.attrib:\n",
    "                node['pos'].append(float(element.attrib['lon']))\n",
    "        \n",
    "        \n",
    "        node['created']={}\n",
    "            \n",
    "        for k,v in element.attrib.iteritems():\n",
    "            if k in CREATED:\n",
    "                node[\"created\"][k]=v\n",
    "            elif k=='k':\n",
    "                pass\n",
    "            elif k!='lon' and k!='lat':\n",
    "                node[k]=v\n",
    "        \n",
    "        \n",
    "        node['address']={}\n",
    "        \n",
    "        for child in element.iter('tag'):\n",
    "            k = child.attrib['k']\n",
    "            v = child.attrib['v']\n",
    "            \n",
    "            if not problemchars.match(k):\n",
    "                if addr.match(k):\n",
    "                    after_addr = k.partition(\"addr:\")[2]\n",
    "                    \n",
    "                    if after_addr == 'postcode':\n",
    "                        node['address'][after_addr] = clean_post_code(v)\n",
    "                    \n",
    "                    elif after_addr == 'city':\n",
    "                        node['address'][after_addr] = clean_city(v)\n",
    "                        \n",
    "                    elif after_addr == 'street':\n",
    "                        node['address'][after_addr] = clean_streen_names(v)\n",
    "                    \n",
    "                    elif ':' not in after_addr:\n",
    "                        try:\n",
    "                            node['address'][after_addr] = v\n",
    "                        except:\n",
    "                            pass\n",
    "                            \n",
    "                else:\n",
    "                    if ':' in k:\n",
    "                        a = k.partition(':')[0] \n",
    "                        b = k.partition(':')[2]\n",
    "                        node[a]={}\n",
    "                        node[a][b] = v\n",
    "                    elif k!='address':\n",
    "                        node[k]=v\n",
    "           \n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_map(file_in, pretty = False):\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return 1\n",
    "\n",
    "def test():\n",
    "  data = process_map('toronto_small.osm', True)\n",
    "      \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5 Loading json File into Database\n",
    "Once the data is cleaned and in a json format it is easy to load into the database. The following command does it on a system that has mongodb setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mongoimport --db test --collection toronto_small --file /path/to/file/toronto_small.osm.json --jsonArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration (find patterns and report interesting facts/stats)\n",
    "Now that the data is cleaned and loaded into the database, it can be explored more easily. The following is a description of some of the data exploration that was done. There is a written explanation as well as mongodb commands and the output from those commands.\n",
    "\n",
    "### 2.1 Size of toronto_small Collection\n",
    "The total size of the collection when stored in mongodb is 163,004,032 bytes or 163 MB. The .osm source file and .json intermediate file are 117 MB and 159 MB respectively. One of the reasons the data is larger in mongodb is that the total size that is shown includes the index which adds some additional data. The size of the .osm and .json files were taken from the local file manager application. The following command is how to get the stats of the collection and the output of the command is included. This command was entered into the mongodb prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "> db.toronto_small.stats()\n",
    "{\n",
    "    \"ns\" : \"test.toronto_small\",\n",
    "    \"count\" : 549272,\n",
    "    \"size\" : 163004032,\n",
    "    \"avgObjSize\" : 296,\n",
    "    \"storageSize\" : 174735360,\n",
    "    \"numExtents\" : 12,\n",
    "    \"nindexes\" : 1,\n",
    "    \"lastExtentSize\" : 50798592,\n",
    "    \"paddingFactor\" : 1,\n",
    "    \"systemFlags\" : 1,\n",
    "    \"userFlags\" : 1,\n",
    "    \"totalIndexSize\" : 17831856,\n",
    "    \"indexSizes\" : {\n",
    "        \"_id_\" : 17831856\n",
    "    },\n",
    "    \"ok\" : 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Number of Unique Users\n",
    "The number of unique users was determined using the following mongodb aggregation pipeline. The output of the following code is:\n",
    "\n",
    "*{u'_id': None, u'unique_users': 1144}*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "\n",
    "def aggregate_unique(collection):\n",
    "    \n",
    "    unique_users = collection.aggregate([\n",
    "        { \"$match\": { \"created.user\": { \"$exists\": True } }},\n",
    "        { \"$group\": { \"_id\": { \"user\": \"$created.user\" }, \"count\": { \"$sum\": 1 } } },\n",
    "        { \"$group\" : { \"_id\" : None, \"unique_users\" : { \"$sum\" : 1 } } }\n",
    "    ])\n",
    "    \n",
    "    return unique_users \n",
    "\n",
    "def test():\n",
    "\n",
    "    client = MongoClient ()\n",
    "    db = client.test\n",
    "    collection = db.toronto_small\n",
    "\n",
    "    unique_users = aggregate_unique(collection)\n",
    "    \n",
    "    for a in unique_users:\n",
    "        pprint.pprint(a)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Number of Nodes\n",
    "The number of nodes was determined using the following mongodb aggregation pipeline. The output of the following code is:\n",
    "\n",
    "{u'_id': None, u'number_of_nodes': 479158}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Note - the main function and import statements have been ommitted for this code and all \n",
    "#the following aggregations\n",
    "\n",
    "def count_nodes(collection):\n",
    "    \n",
    "    nodes = collection.aggregate([\n",
    "        { \"$match\": { \"type\": \"node\" } },\n",
    "        { \"$group\" : { \"_id\" : None, \"number_of_nodes\" : { \"$sum\" : 1 } } }\n",
    "    ])\n",
    "    \n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Number of Ways\n",
    "Making a small change to the code from section 2.3 will return the number of ways. The change to the code is:\n",
    "\n",
    "*{ \"$match\": { \"type\": \"~~node~~ way\" } },*\n",
    "\n",
    "The result of this code is:\n",
    "\n",
    "*{u'_id': None, u'number_of_ways': 69720}*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Types and Numbers of Buildings\n",
    "The aggregation below shows the building types and the number of each type of building in the database.  The output from the aggregation is:\n",
    "\n",
    "*{u'_id': None, u'count': 539437}  \n",
    "{u'_id': u'yes', u'count': 5247}  \n",
    "{u'_id': u'house', u'count': 1177}  \n",
    "{u'_id': u'residential', u'count': 557}  \n",
    "{u'_id': u'retail', u'count': 468}  \n",
    "{u'_id': u'industrial', u'count': 410}  \n",
    "{u'_id': u'apartments', u'count': 391}  \n",
    "{u'_id': {u'levels': u'2'}, u'count': 347}  \n",
    "{u'_id': {u'levels': u'1'}, u'count': 265}  \n",
    "{u'_id': u'school', u'count': 139}  \n",
    "{u'_id': {u'material': u'brick'}, u'count': 120}*\n",
    "\n",
    "Most documents (almost 540,000 out of 550,000) do not have a building tag. The most common building tag is 'yes' which probably means that there is a building. The second most common building tag is 'house' and then 'residential'. This only shows the top 10 most common building tags ($limit is set to 11 because documents without a building tag are also included)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_buildings(collection):\n",
    "    \n",
    "    buildings = collection.aggregate([\n",
    "        { \"$group\" : { \"_id\" : \"$building\", \"count\" : { \"$sum\" : 1 } } },\n",
    "        { \"$sort\" : { \"count\" : -1 } },\n",
    "        { \"$limit\" : 11 }\n",
    "    ])\n",
    "    \n",
    "    return buildings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Types and Numbers of Surfaces\n",
    "The aggregation below shows the surface types and the number of each type of surface in the database.  The output from the aggregation is:\n",
    "\n",
    "*{u'_id': None, u'count': 533671}  \n",
    "{u'_id': u'asphalt', u'count': 14235}  \n",
    "{u'_id': u'paved', u'count': 419}  \n",
    "{u'_id': u'unpaved', u'count': 333}  \n",
    "{u'_id': u'concrete', u'count': 286}  \n",
    "{u'_id': u'gravel', u'count': 85}  \n",
    "{u'_id': u'dirt', u'count': 79}   \n",
    "{u'_id': u'wood', u'count': 42}  \n",
    "{u'_id': u'paving_stones', u'count': 24}  \n",
    "{u'_id': u'grass', u'count': 21}  \n",
    "{u'_id': u'ground', u'count': 19}*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_surfaces(collection):\n",
    "    \n",
    "    surfaces = collection.aggregate([\n",
    "        { \"$group\" : { \"_id\" : \"$surface\", \"count\" : { \"$sum\" : 1 } } },\n",
    "        { \"$sort\" : { \"count\" : -1 } },\n",
    "        { \"$limit\" : 11 }\n",
    "    ])\n",
    "    \n",
    "    return surfaces "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Suggestions for Improving and Analyzing the Data\n",
    "This section suggests improvements in the data structure and analysis and discusses the benefits of these improvements.\n",
    "\n",
    "### 3.1 Improvements\n",
    "#### 3.1.1. Add an 'Obsolete' Field  \n",
    "The 'created' field contains the time that an item was added to the dataset. When an item in the physical world is removed (a store closes, a building is demolished, a highway is relocated), instead of removing it from the database, an 'obsolete' field could be added to the item. \n",
    "#### 3.1.2 Add a Field for User Feedback\n",
    "A user feedback field, like upvotes on Reddit, could be used to validate the quality of the data. A user could upvote a map element that is correctly located and the data is correct and downvote an element that is incorrect.\n",
    "### 3.2 Benefits\n",
    "#### 3.2.1. Add an 'Obsolete' Field\n",
    "The benefit of the 'obsolete' field would make it possible to view the map at a specific point in time and view changes in an area over time. \n",
    "This could be useful for a city planner or a neighbourhood organization  that is trying to study changes in their neighbourhood to shape policies or regulations.\n",
    "#### 3.2.2 Add a Field for User Feedback\n",
    "A user feedback field would allow for very quick data validation. If incorrect data was entered, any user using the map could 'vote' on the validity of the data. A lot of downvotes may indicate suspect data and could be a signal that the data should be verified."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
